{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "rossman_data_clean.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jinyong-20/Machine_Learning_Programming/blob/master/rossman_data_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQaBnl8VTuDC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1jodpfkT22x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9fabbd9d-63f0-450b-fa59-a4bb8ff46dd6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/My Drive/\"\n",
        "base_dir = root_dir + 'fastai-v3/'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhJ23oGbTuDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai.basics import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk74GFaLWDw0",
        "colab_type": "text"
      },
      "source": [
        "\\# 구글 드라이브 마운트, 함수 임포트, 자동 리로드 설정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l687Y1xxTuDH",
        "colab_type": "text"
      },
      "source": [
        "# Rossmann"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIiGIdrKTuDH",
        "colab_type": "text"
      },
      "source": [
        "## Data preparation / Feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8ZwYzkyTuDH",
        "colab_type": "text"
      },
      "source": [
        "In addition to the provided data, we will be using external datasets put together by participants in the Kaggle competition. You can download all of them [here](http://files.fast.ai/part2/lesson14/rossmann.tgz). Then you shold untar them in the dirctory to which `PATH` is pointing below.\n",
        "\n",
        "For completeness, the implementation used to put them together is included below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQOHTlolUKbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path =  Config().data_path()/'rossmann/'\n",
        "path.mkdir(parents=True, exist_ok=True)\n",
        "! mv '/content/googletrend.csv' '/root/.fastai/data/rossmann/'\n",
        "! mv '/content/sample_submission.csv' '/root/.fastai/data/rossmann/'\n",
        "! mv '/content/state_names.csv' '/root/.fastai/data/rossmann/'\n",
        "! mv '/content/store.csv' '/root/.fastai/data/rossmann/'\n",
        "! mv '/content/store_states.csv' '/root/.fastai/data/rossmann/'\n",
        "! mv '/content/test.csv' '/root/.fastai/data/rossmann/'\n",
        "! mv '/content/train.csv' '/root/.fastai/data/rossmann/'\n",
        "! mv '/content/weather.csv' '/root/.fastai/data/rossmann/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rccPN4EHWItk",
        "colab_type": "text"
      },
      "source": [
        "\\# 위의 링크에서 압축파일을 다운로드 한 후, 직접 넣어주고 경로로 이동시켜줌"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44TIe6w8TuDI",
        "colab_type": "code",
        "outputId": "0acd4457-2a9e-4c0f-e93b-99a5c4f71056",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "PATH=Config().data_path()/Path('rossmann/')\n",
        "table_names = ['train', 'store', 'store_states', 'state_names', 'googletrend', 'weather', 'test']\n",
        "tables = [pd.read_csv(PATH/f'{fname}.csv', low_memory=False) for fname in table_names]\n",
        "train, store, store_states, state_names, googletrend, weather, test = tables\n",
        "len(train),len(test)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1017209, 41088)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Nwif31RWOVi",
        "colab_type": "text"
      },
      "source": [
        "\\# 테이블 명을 정하고, csv파일에서 테이블을 따옴"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v52jS4pkTuDK",
        "colab_type": "text"
      },
      "source": [
        "We turn state Holidays to booleans, to make them more convenient for modeling. We can do calculations on pandas fields using notation very similar (often identical) to numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTSEQzwrTuDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.StateHoliday = train.StateHoliday!='0'\n",
        "test.StateHoliday = test.StateHoliday!='0'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndbk7mhHWjLB",
        "colab_type": "text"
      },
      "source": [
        "\\# 편의성을 위한 boolean형 변수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4Dj8h51TuDM",
        "colab_type": "text"
      },
      "source": [
        "`join_df` is a function for joining tables on specific fields. By default, we'll be doing a left outer join of `right` on the `left` argument using the given fields for each table.\n",
        "\n",
        "Pandas does joins using the `merge` method. The `suffixes` argument describes the naming convention for duplicate fields. We've elected to leave the duplicate field names on the left untouched, and append a \"\\_y\" to those on the right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVPMur-FTuDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def join_df(left, right, left_on, right_on=None, suffix='_y'):\n",
        "    if right_on is None: right_on = left_on\n",
        "    return left.merge(right, how='left', left_on=left_on, right_on=right_on, \n",
        "                      suffixes=(\"\", suffix))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3isOniyW2xH",
        "colab_type": "text"
      },
      "source": [
        "\\# 테이블을 합쳐줄 join_df 함수 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ooffsb_RTuDO",
        "colab_type": "text"
      },
      "source": [
        "Join weather/state names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s64HUOMFTuDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weather = join_df(weather, state_names, \"file\", \"StateName\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VlhfgC0W-Ca",
        "colab_type": "text"
      },
      "source": [
        "\\# 합쳐진 테이블 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFfrBVvFTuDQ",
        "colab_type": "text"
      },
      "source": [
        "In pandas you can add new columns to a dataframe by simply defining it. We'll do this for googletrends by extracting dates and state names from the given data and adding those columns.\n",
        "\n",
        "We're also going to replace all instances of state name 'NI' to match the usage in the rest of the data: 'HB,NI'. This is a good opportunity to highlight pandas indexing. We can use `.loc[rows, cols]` to select a list of rows and a list of columns from the dataframe. In this case, we're selecting rows w/ statename 'NI' by using a boolean list `googletrend.State=='NI'` and selecting \"State\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4RY8-VfkIUR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "36f6a64d-2cff-4875-e46b-4d484c6e0ec4"
      },
      "source": [
        "doc(googletrend.loc)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h4 id=\"_LocIndexer object at 0x7fd3f462d1d8>\" class=\"doc_header\"><code>_LocIndexer object at 0x7fd3f462d1d8></code><a class=\"source_link\" data-toggle=\"collapse\" data-target=\"#_LocIndexer object at 0x7fd3f462d1d8>-pytest\" style=\"float:right; padding-right:10px\">[test]</a></h4><blockquote><p><code>_LocIndexer object at 0x7fd3f462d1d8></code>(<strong><code>axis</code></strong>=<strong><em><code>None</code></em></strong>)</p>\n",
              "</blockquote>\n",
              "<div class=\"collapse\" id=\"_LocIndexer object at 0x7fd3f462d1d8>-pytest\"><div class=\"card card-body pytest_card\"><a type=\"button\" data-toggle=\"collapse\" data-target=\"#_LocIndexer object at 0x7fd3f462d1d8>-pytest\" class=\"close\" aria-label=\"Close\"><span aria-hidden=\"true\">&times;</span></a><p>No tests found for <code>_LocIndexer object at 0x7fd3f462d1d8&gt;</code>. To contribute a test please refer to <a href=\"/dev/test.html\">this guide</a> and <a href=\"https://forums.fast.ai/t/improving-expanding-functional-tests/32929\">this discussion</a>.</p></div></div><p>Access a group of rows and columns by label(s) or a boolean array. <code>.loc[]</code> is primarily label based, but may also be used with a\n",
              "boolean array.</p>\n",
              "<p>Allowed inputs are:</p>\n",
              "<ul>\n",
              "<li>A single label, e.g. <code>5</code> or <code>'a'</code>, (note that <code>5</code> is\n",
              "interpreted as a <em>label</em> of the index, and <strong>never</strong> as an\n",
              "integer position along the index).</li>\n",
              "<li>A list or array of labels, e.g. <code>['a', 'b', 'c']</code>.</li>\n",
              "<li><p>A slice object with labels, e.g. <code>'a':'f'</code>.</p>\n",
              "<p>.. warning:: Note that contrary to usual python slices, <strong>both</strong> the</p>\n",
              "\n",
              "<pre><code>start and the stop are included</code></pre>\n",
              "</li>\n",
              "<li><p>A boolean array of the same length as the axis being sliced,\n",
              "e.g. <code>[True, False, True]</code>.</p>\n",
              "</li>\n",
              "<li>A <code>callable</code> function with one argument (the calling Series or\n",
              "DataFrame) and that returns valid output for indexing (one of the above)</li>\n",
              "</ul>\n",
              "<p>See more at :ref:<code>Selection by Label &lt;indexing.label&gt;</code></p>\n",
              "<h2 id=\"Raises\">Raises<a class=\"anchor-link\" href=\"#Raises\">&#182;</a></h2><p>KeyError\n",
              "    If any items are not found.</p>\n",
              "<h2 id=\"See-Also\">See Also<a class=\"anchor-link\" href=\"#See-Also\">&#182;</a></h2><p>DataFrame.at : Access a single value for a row/column label pair.\n",
              "DataFrame.iloc : Access group of rows and columns by integer position(s).\n",
              "DataFrame.xs : Returns a cross-section (row(s) or column(s)) from the\n",
              "    Series/DataFrame.\n",
              "Series.loc : Access group of values using labels.</p>\n",
              "<h2 id=\"Examples\">Examples<a class=\"anchor-link\" href=\"#Examples\">&#182;</a></h2><p><strong>Getting values</strong></p>\n",
              "<blockquote><blockquote><blockquote><p>df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n",
              "...      index=['cobra', 'viper', 'sidewinder'],\n",
              "...      columns=['max_speed', 'shield'])\n",
              "df\n",
              "            max_speed  shield\n",
              "cobra               1       2\n",
              "viper               4       5\n",
              "sidewinder          7       8</p>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "<p>Single label. Note this returns the row as a Series.</p>\n",
              "<blockquote><blockquote><blockquote><p>df.loc['viper']\n",
              "max_speed    4\n",
              "shield       5\n",
              "Name: viper, dtype: int64</p>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "<p>List of labels. Note using <code>[[]]</code> returns a DataFrame.</p>\n",
              "<blockquote><blockquote><blockquote><p>df.loc[['viper', 'sidewinder']]\n",
              "            max_speed  shield\n",
              "viper               4       5\n",
              "sidewinder          7       8</p>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "<p>Single label for row and column</p>\n",
              "<blockquote><blockquote><blockquote><p>df.loc['cobra', 'shield']\n",
              "2</p>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "<p>Slice with labels for row and single label for column. As mentioned\n",
              "above, note that both the start and stop of the slice are included.</p>\n",
              "<blockquote><blockquote><blockquote><p>df.loc['cobra':'viper', 'max_speed']\n",
              "cobra    1\n",
              "viper    4\n",
              "Name: max_speed, dtype: int64</p>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "<p>Boolean list with the same length as the row axis</p>\n",
              "<blockquote><blockquote><blockquote><p>df.loc[[False, False, True]]\n",
              "            max_speed  shield\n",
              "sidewinder          7       8</p>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "<p>Conditional that returns a boolean Series</p>\n",
              "<blockquote><blockquote><blockquote><p>df.loc[df['shield'] &gt; 6]\n",
              "            max_speed  shield\n",
              "sidewinder          7       8</p>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "<p>Conditional that returns a boolean Series with column labels specified</p>\n",
              "<blockquote><blockquote><blockquote><p>df.loc[df['shield'] &gt; 6, ['max_speed']]\n",
              "            max_speed\n",
              "sidewinder          7</p>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "<p>Callable that returns a boolean Series</p>\n",
              "<blockquote><blockquote><blockquote><p>df.loc[lambda df: df['shield'] == 8]\n",
              "            max_speed  shield\n",
              "sidewinder          7       8</p>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "<p><strong>Setting values</strong></p>\n",
              "<p>Set value for all items matching the list of labels</p>\n",
              "<blockquote><blockquote><blockquote><p>df.loc[['viper', 'sidewinder'], ['shield']] = 50\n",
              "df\n",
              "            max_speed  shield\n",
              "cobra               1       2\n",
              "viper               4      50\n",
              "sidewinder          7      50</p>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "<p>Set value for an entire row</p>\n",
              "<blockquote><blockquote><blockquote><p>df.loc['cobra'] = 10\n",
              "df\n",
              "            max_speed  shield\n",
              "cobra              10      10\n",
              "viper               4      50\n",
              "sidewinder          7      50</p>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "<p>Set value for an entire column</p>\n",
              "<blockquote><blockquote><blockquote><p>df.loc[:, 'max_speed'] = 30\n",
              "df\n",
              "            max_speed  shield\n",
              "cobra              30      10\n",
              "viper              30      50\n",
              "sidewinder         30      50</p>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "<p>Set value for rows matching callable condition</p>\n",
              "<blockquote><blockquote><blockquote><p>df.loc[df['shield'] &gt; 35] = 0\n",
              "df\n",
              "            max_speed  shield\n",
              "cobra              30      10\n",
              "viper               0       0\n",
              "sidewinder          0       0</p>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "<p><strong>Getting values on a DataFrame with an index that has integer labels</strong></p>\n",
              "<p>Another example using integers for the index</p>\n",
              "<blockquote><blockquote><blockquote><p>df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n",
              "...      index=[7, 8, 9], columns=['max_speed', 'shield'])\n",
              "df\n",
              "   max_speed  shield\n",
              "7          1       2\n",
              "8          4       5\n",
              "9          7       8</p>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "<p>Slice with integer labels for rows. As mentioned above, note that both\n",
              "the start and stop of the slice are included.</p>\n",
              "<blockquote><blockquote><blockquote><p>df.loc[7:9]\n",
              "   max_speed  shield\n",
              "7          1       2\n",
              "8          4       5\n",
              "9          7       8</p>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "<p><strong>Getting values with a MultiIndex</strong></p>\n",
              "<p>A number of examples using a DataFrame with a MultiIndex</p>\n",
              "<blockquote><blockquote><blockquote><p>tuples = [\n",
              "...    ('cobra', 'mark i'), ('cobra', 'mark ii'),\n",
              "...    ('sidewinder', 'mark i'), ('sidewinder', 'mark ii'),\n",
              "...    ('viper', 'mark ii'), ('viper', 'mark iii')\n",
              "... ]\n",
              "index = pd.MultiIndex.from_tuples(tuples)\n",
              "values = [[12, 2], [0, 4], [10, 20],\n",
              "...         [1, 4], [7, 1], [16, 36]]\n",
              "df = pd.DataFrame(values, columns=['max_speed', 'shield'], index=index)\n",
              "df\n",
              "                     max_speed  shield\n",
              "cobra      mark i           12       2\n",
              "           mark ii           0       4\n",
              "sidewinder mark i           10      20\n",
              "           mark ii           1       4\n",
              "viper      mark ii           7       1\n",
              "           mark iii         16      36</p>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "<p>Single label. Note this returns a DataFrame with a single index.</p>\n",
              "<blockquote><blockquote><blockquote><p>df.loc['cobra']\n",
              "         max_speed  shield\n",
              "mark i          12       2\n",
              "mark ii          0       4</p>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "<p>Single index tuple. Note this returns a Series.</p>\n",
              "<blockquote><blockquote><blockquote><p>df.loc[('cobra', 'mark ii')]\n",
              "max_speed    0\n",
              "shield       4\n",
              "Name: (cobra, mark ii), dtype: int64</p>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "<p>Single label for row and column. Similar to passing in a tuple, this\n",
              "returns a Series.</p>\n",
              "<blockquote><blockquote><blockquote><p>df.loc['cobra', 'mark i']\n",
              "max_speed    12\n",
              "shield        2\n",
              "Name: (cobra, mark i), dtype: int64</p>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "<p>Single tuple. Note using <code>[[]]</code> returns a DataFrame.</p>\n",
              "<blockquote><blockquote><blockquote><p>df.loc[[('cobra', 'mark ii')]]\n",
              "               max_speed  shield\n",
              "cobra mark ii          0       4</p>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "<p>Single tuple for the index with a single label for the column</p>\n",
              "<blockquote><blockquote><blockquote><p>df.loc[('cobra', 'mark i'), 'shield']\n",
              "2</p>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "<p>Slice from index tuple to single label</p>\n",
              "<blockquote><blockquote><blockquote><p>df.loc[('cobra', 'mark i'):'viper']\n",
              "                     max_speed  shield\n",
              "cobra      mark i           12       2\n",
              "           mark ii           0       4\n",
              "sidewinder mark i           10      20\n",
              "           mark ii           1       4\n",
              "viper      mark ii           7       1\n",
              "           mark iii         16      36</p>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "<p>Slice from index tuple to index tuple</p>\n",
              "<blockquote><blockquote><blockquote><p>df.loc[('cobra', 'mark i'):('viper', 'mark ii')]\n",
              "                    max_speed  shield\n",
              "cobra      mark i          12       2\n",
              "           mark ii          0       4\n",
              "sidewinder mark i          10      20\n",
              "           mark ii          1       4\n",
              "viper      mark ii          7       1</p>\n",
              "</blockquote>\n",
              "</blockquote>\n",
              "</blockquote>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uf4BgHNTuDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "googletrend['Date'] = googletrend.week.str.split(' - ', expand=True)[0]\n",
        "googletrend['State'] = googletrend.file.str.split('_', expand=True)[2]\n",
        "googletrend.loc[googletrend.State=='NI', \"State\"] = 'HB,NI'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RppJrLtTXY3L",
        "colab_type": "text"
      },
      "source": [
        "\\# 테이블의 내용을 수정 및 추가"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYfO1eIfTuDS",
        "colab_type": "text"
      },
      "source": [
        "The following extracts particular date fields from a complete datetime for the purpose of constructing categoricals.\n",
        "\n",
        "You should *always* consider this feature extraction step when working with date-time. Without expanding your date-time into these additional fields, you can't capture any trend/cyclical behavior as a function of time at any of these granularities. We'll add to every table with a date field."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6qFiGdTTuDT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_datepart(df, fldname, drop=True, time=False):\n",
        "    \"Helper function that adds columns relevant to a date.\"\n",
        "    fld = df[fldname]\n",
        "    fld_dtype = fld.dtype\n",
        "    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n",
        "        fld_dtype = np.datetime64\n",
        "\n",
        "    if not np.issubdtype(fld_dtype, np.datetime64):\n",
        "        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n",
        "    targ_pre = re.sub('[Dd]ate$', '', fldname)\n",
        "    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n",
        "            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
        "    if time: attr = attr + ['Hour', 'Minute', 'Second']\n",
        "    for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())\n",
        "    df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9\n",
        "    if drop: df.drop(fldname, axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOsU2fR_Yf3q",
        "colab_type": "text"
      },
      "source": [
        "\\# 날짜에서 특정 부분을 추출하는 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JrldIclTuDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "add_datepart(weather, \"Date\", drop=False)\n",
        "add_datepart(googletrend, \"Date\", drop=False)\n",
        "add_datepart(train, \"Date\", drop=False)\n",
        "add_datepart(test, \"Date\", drop=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CQodIxpekdl",
        "colab_type": "text"
      },
      "source": [
        "\\# 함수 적용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsL7-phXTuDW",
        "colab_type": "text"
      },
      "source": [
        "The Google trends data has a special category for the whole of the Germany - we'll pull that out so we can use it explicitly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF2Ryr4iTuDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trend_de = googletrend[googletrend.file == 'Rossmann_DE']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ9axFY4TuDY",
        "colab_type": "text"
      },
      "source": [
        "Now we can outer join all of our data into a single dataframe. Recall that in outer joins everytime a value in the joining field on the left table does not have a corresponding value on the right table, the corresponding row in the new table has Null values for all right table fields. One way to check that all records are consistent and complete is to check for Null values post-join, as we do here.\n",
        "\n",
        "*Aside*: Why not just do an inner join?\n",
        "If you are assuming that all records are complete and match on the field you desire, an inner join will do the same thing as an outer join. However, in the event you are wrong or a mistake is made, an outer join followed by a null-check will catch it. (Comparing before/after # of rows for inner join is equivalent, but requires keeping track of before/after row #'s. Outer join is easier.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vbk7W4HDTuDY",
        "colab_type": "code",
        "outputId": "86d37045-28c7-40ea-b4e5-c0b93db49ee4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "store = join_df(store, store_states, \"Store\")\n",
        "len(store[store.State.isnull()])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csZnOgZJTuDa",
        "colab_type": "code",
        "outputId": "cb44ed2c-47b4-48fb-c17e-e4546e2db172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "joined = join_df(train, store, \"Store\")\n",
        "joined_test = join_df(test, store, \"Store\")\n",
        "len(joined[joined.StoreType.isnull()]),len(joined_test[joined_test.StoreType.isnull()])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXMj5jyMTuDb",
        "colab_type": "code",
        "outputId": "c26fb0b9-3b58-402c-bc1b-2a67109a7949",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "joined = join_df(joined, googletrend, [\"State\",\"Year\", \"Week\"])\n",
        "joined_test = join_df(joined_test, googletrend, [\"State\",\"Year\", \"Week\"])\n",
        "len(joined[joined.trend.isnull()]),len(joined_test[joined_test.trend.isnull()])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhgwePcWTuDd",
        "colab_type": "code",
        "outputId": "e271f01e-3ec1-4fe5-dc67-25012a49b6a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "joined = joined.merge(trend_de, 'left', [\"Year\", \"Week\"], suffixes=('', '_DE'))\n",
        "joined_test = joined_test.merge(trend_de, 'left', [\"Year\", \"Week\"], suffixes=('', '_DE'))\n",
        "len(joined[joined.trend_DE.isnull()]),len(joined_test[joined_test.trend_DE.isnull()])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1pS_fg5TuDf",
        "colab_type": "code",
        "outputId": "63c779f9-ca51-4206-97f9-03447bdbbfa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "joined = join_df(joined, weather, [\"State\",\"Date\"])\n",
        "joined_test = join_df(joined_test, weather, [\"State\",\"Date\"])\n",
        "len(joined[joined.Mean_TemperatureC.isnull()]),len(joined_test[joined_test.Mean_TemperatureC.isnull()])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1uCQtEwgY_y",
        "colab_type": "text"
      },
      "source": [
        "\\# 하나의 테이블(joined_test, joined)에 모두 결합하고 널 값의 개수를 출력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_WT1kRCTuDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for df in (joined, joined_test):\n",
        "    for c in df.columns:\n",
        "        if c.endswith('_y'):\n",
        "            if c in df.columns: df.drop(c, inplace=True, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Oa5NTqIfRvx",
        "colab_type": "text"
      },
      "source": [
        "\\# 테이블에서 suffix로 끝나는 colums을 지움(drop)\n",
        "\n",
        "Q : inplace 인자를 true로 설정했을때의 차이점이 뭔가요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjkjnR7bhza9",
        "colab_type": "code",
        "outputId": "0b086cae-bb65-4008-f905-11306feec68e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "help(df.drop)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on method drop in module pandas.core.frame:\n",
            "\n",
            "drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise') method of pandas.core.frame.DataFrame instance\n",
            "    Drop specified labels from rows or columns.\n",
            "    \n",
            "    Remove rows or columns by specifying label names and corresponding\n",
            "    axis, or by specifying directly index or column names. When using a\n",
            "    multi-index, labels on different levels can be removed by specifying\n",
            "    the level.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    labels : single label or list-like\n",
            "        Index or column labels to drop.\n",
            "    axis : {0 or 'index', 1 or 'columns'}, default 0\n",
            "        Whether to drop labels from the index (0 or 'index') or\n",
            "        columns (1 or 'columns').\n",
            "    index : single label or list-like\n",
            "        Alternative to specifying axis (``labels, axis=0``\n",
            "        is equivalent to ``index=labels``).\n",
            "    \n",
            "        .. versionadded:: 0.21.0\n",
            "    columns : single label or list-like\n",
            "        Alternative to specifying axis (``labels, axis=1``\n",
            "        is equivalent to ``columns=labels``).\n",
            "    \n",
            "        .. versionadded:: 0.21.0\n",
            "    level : int or level name, optional\n",
            "        For MultiIndex, level from which the labels will be removed.\n",
            "    inplace : bool, default False\n",
            "        If True, do operation inplace and return None.\n",
            "    errors : {'ignore', 'raise'}, default 'raise'\n",
            "        If 'ignore', suppress error and only existing labels are\n",
            "        dropped.\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    DataFrame\n",
            "        DataFrame without the removed index or column labels.\n",
            "    \n",
            "    Raises\n",
            "    ------\n",
            "    KeyError\n",
            "        If any of the labels is not found in the selected axis.\n",
            "    \n",
            "    See Also\n",
            "    --------\n",
            "    DataFrame.loc : Label-location based indexer for selection by label.\n",
            "    DataFrame.dropna : Return DataFrame with labels on given axis omitted\n",
            "        where (all or any) data are missing.\n",
            "    DataFrame.drop_duplicates : Return DataFrame with duplicate rows\n",
            "        removed, optionally only considering certain columns.\n",
            "    Series.drop : Return Series with specified index labels removed.\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> df = pd.DataFrame(np.arange(12).reshape(3, 4),\n",
            "    ...                   columns=['A', 'B', 'C', 'D'])\n",
            "    >>> df\n",
            "       A  B   C   D\n",
            "    0  0  1   2   3\n",
            "    1  4  5   6   7\n",
            "    2  8  9  10  11\n",
            "    \n",
            "    Drop columns\n",
            "    \n",
            "    >>> df.drop(['B', 'C'], axis=1)\n",
            "       A   D\n",
            "    0  0   3\n",
            "    1  4   7\n",
            "    2  8  11\n",
            "    \n",
            "    >>> df.drop(columns=['B', 'C'])\n",
            "       A   D\n",
            "    0  0   3\n",
            "    1  4   7\n",
            "    2  8  11\n",
            "    \n",
            "    Drop a row by index\n",
            "    \n",
            "    >>> df.drop([0, 1])\n",
            "       A  B   C   D\n",
            "    2  8  9  10  11\n",
            "    \n",
            "    Drop columns and/or rows of MultiIndex DataFrame\n",
            "    \n",
            "    >>> midx = pd.MultiIndex(levels=[['lama', 'cow', 'falcon'],\n",
            "    ...                              ['speed', 'weight', 'length']],\n",
            "    ...                      codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2],\n",
            "    ...                             [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n",
            "    >>> df = pd.DataFrame(index=midx, columns=['big', 'small'],\n",
            "    ...                   data=[[45, 30], [200, 100], [1.5, 1], [30, 20],\n",
            "    ...                         [250, 150], [1.5, 0.8], [320, 250],\n",
            "    ...                         [1, 0.8], [0.3, 0.2]])\n",
            "    >>> df\n",
            "                    big     small\n",
            "    lama    speed   45.0    30.0\n",
            "            weight  200.0   100.0\n",
            "            length  1.5     1.0\n",
            "    cow     speed   30.0    20.0\n",
            "            weight  250.0   150.0\n",
            "            length  1.5     0.8\n",
            "    falcon  speed   320.0   250.0\n",
            "            weight  1.0     0.8\n",
            "            length  0.3     0.2\n",
            "    \n",
            "    >>> df.drop(index='cow', columns='small')\n",
            "                    big\n",
            "    lama    speed   45.0\n",
            "            weight  200.0\n",
            "            length  1.5\n",
            "    falcon  speed   320.0\n",
            "            weight  1.0\n",
            "            length  0.3\n",
            "    \n",
            "    >>> df.drop(index='length', level=1)\n",
            "                    big     small\n",
            "    lama    speed   45.0    30.0\n",
            "            weight  200.0   100.0\n",
            "    cow     speed   30.0    20.0\n",
            "            weight  250.0   150.0\n",
            "    falcon  speed   320.0   250.0\n",
            "            weight  1.0     0.8\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PONKinOiih9z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "583d856c-4bc9-481d-9d8c-003b47c219f9"
      },
      "source": [
        "help(c.endswith)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on built-in function endswith:\n",
            "\n",
            "endswith(...) method of builtins.str instance\n",
            "    S.endswith(suffix[, start[, end]]) -> bool\n",
            "    \n",
            "    Return True if S ends with the specified suffix, False otherwise.\n",
            "    With optional start, test S beginning at that position.\n",
            "    With optional end, stop comparing S at that position.\n",
            "    suffix can also be a tuple of strings to try.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PA6a0lQyTuDi",
        "colab_type": "text"
      },
      "source": [
        "Next we'll fill in missing values to avoid complications with `NA`'s. `NA` (not available) is how Pandas indicates missing values; many models have problems when missing values are present, so it's always important to think about how to deal with them. In these cases, we are picking an arbitrary *signal value* that doesn't otherwise appear in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ4y-6oPTuDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for df in (joined,joined_test):\n",
        "    df['CompetitionOpenSinceYear'] = df.CompetitionOpenSinceYear.fillna(1900).astype(np.int32)\n",
        "    df['CompetitionOpenSinceMonth'] = df.CompetitionOpenSinceMonth.fillna(1).astype(np.int32)\n",
        "    df['Promo2SinceYear'] = df.Promo2SinceYear.fillna(1900).astype(np.int32)\n",
        "    df['Promo2SinceWeek'] = df.Promo2SinceWeek.fillna(1).astype(np.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZfdwztXjg_q",
        "colab_type": "text"
      },
      "source": [
        "\\# missing value에 임의의 신호값을 넣어줌(데이터에 표시되지는 않는다)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfMRQINwTuDk",
        "colab_type": "text"
      },
      "source": [
        "Next we'll extract features \"CompetitionOpenSince\" and \"CompetitionDaysOpen\". Note the use of `apply()` in mapping a function across dataframe values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhTlq482TuDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for df in (joined,joined_test):\n",
        "    df[\"CompetitionOpenSince\"] = pd.to_datetime(dict(year=df.CompetitionOpenSinceYear, \n",
        "                                                     month=df.CompetitionOpenSinceMonth, day=15))\n",
        "    df[\"CompetitionDaysOpen\"] = df.Date.subtract(df.CompetitionOpenSince).dt.days"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu0omC_8j3A6",
        "colab_type": "text"
      },
      "source": [
        "\\# 년, 월에 대한 항목을 사전형으로 추출"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW8ihkWGTuDm",
        "colab_type": "text"
      },
      "source": [
        "We'll replace some erroneous / outlying data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb9MjPPdTuDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for df in (joined,joined_test):\n",
        "    df.loc[df.CompetitionDaysOpen<0, \"CompetitionDaysOpen\"] = 0\n",
        "    df.loc[df.CompetitionOpenSinceYear<1990, \"CompetitionDaysOpen\"] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl3ykOEmj_Wr",
        "colab_type": "text"
      },
      "source": [
        "\\# 0보다 날짜가 작거나, 년도가 1990년 이전인 경우에 0을 대입\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbgm_S45TuDn",
        "colab_type": "text"
      },
      "source": [
        "We add \"CompetitionMonthsOpen\" field, limiting the maximum to 2 years to limit number of unique categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow-CYombTuDo",
        "colab_type": "code",
        "outputId": "15a77b84-6c8a-4728-ac65-15613bac4fba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for df in (joined,joined_test):\n",
        "    df[\"CompetitionMonthsOpen\"] = df[\"CompetitionDaysOpen\"]//30\n",
        "    df.loc[df.CompetitionMonthsOpen>24, \"CompetitionMonthsOpen\"] = 24\n",
        "joined.CompetitionMonthsOpen.unique()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([24,  3, 19,  9,  0, 16, 17,  7, 15, 22, 11, 13,  2, 23, 12,  4, 10,  1, 14, 20,  8, 18,  6, 21,  5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxeVE3s-kcqP",
        "colab_type": "text"
      },
      "source": [
        "\\# 월을 추가하고, 월이 24(2년)을 넘으면 24를 대입"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5-cBzZ2TuDp",
        "colab_type": "text"
      },
      "source": [
        "Same process for Promo dates. You may need to install the `isoweek` package first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeB52LXhTuDp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b4398bfc-0d78-429f-c1e1-adcfa3855dfb"
      },
      "source": [
        "# If needed, uncomment:\n",
        "! pip install isoweek"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: isoweek in /usr/local/lib/python3.6/dist-packages (1.3.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcGJ0hfplikn",
        "colab_type": "text"
      },
      "source": [
        "\\# isoweek패키지 설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj2d6uxsloJF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9005b240-23a1-48c5-90be-e10e1eecce99"
      },
      "source": [
        "help(df.apply)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on method apply in module pandas.core.frame:\n",
            "\n",
            "apply(func, axis=0, raw=False, result_type=None, args=(), **kwds) method of pandas.core.frame.DataFrame instance\n",
            "    Apply a function along an axis of the DataFrame.\n",
            "    \n",
            "    Objects passed to the function are Series objects whose index is\n",
            "    either the DataFrame's index (``axis=0``) or the DataFrame's columns\n",
            "    (``axis=1``). By default (``result_type=None``), the final return type\n",
            "    is inferred from the return type of the applied function. Otherwise,\n",
            "    it depends on the `result_type` argument.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    func : function\n",
            "        Function to apply to each column or row.\n",
            "    axis : {0 or 'index', 1 or 'columns'}, default 0\n",
            "        Axis along which the function is applied:\n",
            "    \n",
            "        * 0 or 'index': apply function to each column.\n",
            "        * 1 or 'columns': apply function to each row.\n",
            "    \n",
            "    raw : bool, default False\n",
            "        Determines if row or column is passed as a Series or ndarray object:\n",
            "    \n",
            "        * ``False`` : passes each row or column as a Series to the\n",
            "          function.\n",
            "        * ``True`` : the passed function will receive ndarray objects\n",
            "          instead.\n",
            "          If you are just applying a NumPy reduction function this will\n",
            "          achieve much better performance.\n",
            "    \n",
            "    result_type : {'expand', 'reduce', 'broadcast', None}, default None\n",
            "        These only act when ``axis=1`` (columns):\n",
            "    \n",
            "        * 'expand' : list-like results will be turned into columns.\n",
            "        * 'reduce' : returns a Series if possible rather than expanding\n",
            "          list-like results. This is the opposite of 'expand'.\n",
            "        * 'broadcast' : results will be broadcast to the original shape\n",
            "          of the DataFrame, the original index and columns will be\n",
            "          retained.\n",
            "    \n",
            "        The default behaviour (None) depends on the return value of the\n",
            "        applied function: list-like results will be returned as a Series\n",
            "        of those. However if the apply function returns a Series these\n",
            "        are expanded to columns.\n",
            "    \n",
            "        .. versionadded:: 0.23.0\n",
            "    \n",
            "    args : tuple\n",
            "        Positional arguments to pass to `func` in addition to the\n",
            "        array/series.\n",
            "    **kwds\n",
            "        Additional keyword arguments to pass as keywords arguments to\n",
            "        `func`.\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    Series or DataFrame\n",
            "        Result of applying ``func`` along the given axis of the\n",
            "        DataFrame.\n",
            "    \n",
            "    See Also\n",
            "    --------\n",
            "    DataFrame.applymap: For elementwise operations.\n",
            "    DataFrame.aggregate: Only perform aggregating type operations.\n",
            "    DataFrame.transform: Only perform transforming type operations.\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    \n",
            "    >>> df = pd.DataFrame([[4, 9]] * 3, columns=['A', 'B'])\n",
            "    >>> df\n",
            "       A  B\n",
            "    0  4  9\n",
            "    1  4  9\n",
            "    2  4  9\n",
            "    \n",
            "    Using a numpy universal function (in this case the same as\n",
            "    ``np.sqrt(df)``):\n",
            "    \n",
            "    >>> df.apply(np.sqrt)\n",
            "         A    B\n",
            "    0  2.0  3.0\n",
            "    1  2.0  3.0\n",
            "    2  2.0  3.0\n",
            "    \n",
            "    Using a reducing function on either axis\n",
            "    \n",
            "    >>> df.apply(np.sum, axis=0)\n",
            "    A    12\n",
            "    B    27\n",
            "    dtype: int64\n",
            "    \n",
            "    >>> df.apply(np.sum, axis=1)\n",
            "    0    13\n",
            "    1    13\n",
            "    2    13\n",
            "    dtype: int64\n",
            "    \n",
            "    Returning a list-like will result in a Series\n",
            "    \n",
            "    >>> df.apply(lambda x: [1, 2], axis=1)\n",
            "    0    [1, 2]\n",
            "    1    [1, 2]\n",
            "    2    [1, 2]\n",
            "    dtype: object\n",
            "    \n",
            "    Passing result_type='expand' will expand list-like results\n",
            "    to columns of a Dataframe\n",
            "    \n",
            "    >>> df.apply(lambda x: [1, 2], axis=1, result_type='expand')\n",
            "       0  1\n",
            "    0  1  2\n",
            "    1  1  2\n",
            "    2  1  2\n",
            "    \n",
            "    Returning a Series inside the function is similar to passing\n",
            "    ``result_type='expand'``. The resulting column names\n",
            "    will be the Series index.\n",
            "    \n",
            "    >>> df.apply(lambda x: pd.Series([1, 2], index=['foo', 'bar']), axis=1)\n",
            "       foo  bar\n",
            "    0    1    2\n",
            "    1    1    2\n",
            "    2    1    2\n",
            "    \n",
            "    Passing ``result_type='broadcast'`` will ensure the same shape\n",
            "    result, whether list-like or scalar is returned by the function,\n",
            "    and broadcast it along the axis. The resulting column names will\n",
            "    be the originals.\n",
            "    \n",
            "    >>> df.apply(lambda x: [1, 2], axis=1, result_type='broadcast')\n",
            "       A  B\n",
            "    0  1  2\n",
            "    1  1  2\n",
            "    2  1  2\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVjXoi1LmX5b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "082d850d-7fe0-40f5-b8e0-ae2266a1531f"
      },
      "source": [
        "help(pd.to_datetime)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on function to_datetime in module pandas.core.tools.datetimes:\n",
            "\n",
            "to_datetime(arg, errors='raise', dayfirst=False, yearfirst=False, utc=None, format=None, exact=True, unit=None, infer_datetime_format=False, origin='unix', cache=True)\n",
            "    Convert argument to datetime.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    arg : int, float, str, datetime, list, tuple, 1-d array, Series DataFrame/dict-like\n",
            "        The object to convert to a datetime.\n",
            "    errors : {'ignore', 'raise', 'coerce'}, default 'raise'\n",
            "        - If 'raise', then invalid parsing will raise an exception.\n",
            "        - If 'coerce', then invalid parsing will be set as NaT.\n",
            "        - If 'ignore', then invalid parsing will return the input.\n",
            "    dayfirst : bool, default False\n",
            "        Specify a date parse order if `arg` is str or its list-likes.\n",
            "        If True, parses dates with the day first, eg 10/11/12 is parsed as\n",
            "        2012-11-10.\n",
            "        Warning: dayfirst=True is not strict, but will prefer to parse\n",
            "        with day first (this is a known bug, based on dateutil behavior).\n",
            "    yearfirst : bool, default False\n",
            "        Specify a date parse order if `arg` is str or its list-likes.\n",
            "    \n",
            "        - If True parses dates with the year first, eg 10/11/12 is parsed as\n",
            "          2010-11-12.\n",
            "        - If both dayfirst and yearfirst are True, yearfirst is preceded (same\n",
            "          as dateutil).\n",
            "    \n",
            "        Warning: yearfirst=True is not strict, but will prefer to parse\n",
            "        with year first (this is a known bug, based on dateutil behavior).\n",
            "    utc : bool, default None\n",
            "        Return UTC DatetimeIndex if True (converting any tz-aware\n",
            "        datetime.datetime objects as well).\n",
            "    format : str, default None\n",
            "        The strftime to parse time, eg \"%d/%m/%Y\", note that \"%f\" will parse\n",
            "        all the way up to nanoseconds.\n",
            "        See strftime documentation for more information on choices:\n",
            "        https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior.\n",
            "    exact : bool, True by default\n",
            "        Behaves as:\n",
            "        - If True, require an exact format match.\n",
            "        - If False, allow the format to match anywhere in the target string.\n",
            "    \n",
            "    unit : str, default 'ns'\n",
            "        The unit of the arg (D,s,ms,us,ns) denote the unit, which is an\n",
            "        integer or float number. This will be based off the origin.\n",
            "        Example, with unit='ms' and origin='unix' (the default), this\n",
            "        would calculate the number of milliseconds to the unix epoch start.\n",
            "    infer_datetime_format : bool, default False\n",
            "        If True and no `format` is given, attempt to infer the format of the\n",
            "        datetime strings, and if it can be inferred, switch to a faster\n",
            "        method of parsing them. In some cases this can increase the parsing\n",
            "        speed by ~5-10x.\n",
            "    origin : scalar, default 'unix'\n",
            "        Define the reference date. The numeric values would be parsed as number\n",
            "        of units (defined by `unit`) since this reference date.\n",
            "    \n",
            "        - If 'unix' (or POSIX) time; origin is set to 1970-01-01.\n",
            "        - If 'julian', unit must be 'D', and origin is set to beginning of\n",
            "          Julian Calendar. Julian day number 0 is assigned to the day starting\n",
            "          at noon on January 1, 4713 BC.\n",
            "        - If Timestamp convertible, origin is set to Timestamp identified by\n",
            "          origin.\n",
            "    cache : bool, default True\n",
            "        If True, use a cache of unique, converted dates to apply the datetime\n",
            "        conversion. May produce significant speed-up when parsing duplicate\n",
            "        date strings, especially ones with timezone offsets. The cache is only\n",
            "        used when there are at least 50 values. The presence of out-of-bounds\n",
            "        values will render the cache unusable and may slow down parsing.\n",
            "    \n",
            "        .. versionadded:: 0.23.0\n",
            "    \n",
            "        .. versionchanged:: 0.25.0\n",
            "            - changed default value from False to True.\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    datetime\n",
            "        If parsing succeeded.\n",
            "        Return type depends on input:\n",
            "    \n",
            "        - list-like: DatetimeIndex\n",
            "        - Series: Series of datetime64 dtype\n",
            "        - scalar: Timestamp\n",
            "    \n",
            "        In case when it is not possible to return designated types (e.g. when\n",
            "        any element of input is before Timestamp.min or after Timestamp.max)\n",
            "        return will have datetime.datetime type (or corresponding\n",
            "        array/Series).\n",
            "    \n",
            "    See Also\n",
            "    --------\n",
            "    DataFrame.astype : Cast argument to a specified dtype.\n",
            "    to_timedelta : Convert argument to timedelta.\n",
            "    convert_dtypes : Convert dtypes.\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    Assembling a datetime from multiple columns of a DataFrame. The keys can be\n",
            "    common abbreviations like ['year', 'month', 'day', 'minute', 'second',\n",
            "    'ms', 'us', 'ns']) or plurals of the same\n",
            "    \n",
            "    >>> df = pd.DataFrame({'year': [2015, 2016],\n",
            "    ...                    'month': [2, 3],\n",
            "    ...                    'day': [4, 5]})\n",
            "    >>> pd.to_datetime(df)\n",
            "    0   2015-02-04\n",
            "    1   2016-03-05\n",
            "    dtype: datetime64[ns]\n",
            "    \n",
            "    If a date does not meet the `timestamp limitations\n",
            "    <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n",
            "    #timeseries-timestamp-limits>`_, passing errors='ignore'\n",
            "    will return the original input instead of raising any exception.\n",
            "    \n",
            "    Passing errors='coerce' will force an out-of-bounds date to NaT,\n",
            "    in addition to forcing non-dates (or non-parseable dates) to NaT.\n",
            "    \n",
            "    >>> pd.to_datetime('13000101', format='%Y%m%d', errors='ignore')\n",
            "    datetime.datetime(1300, 1, 1, 0, 0)\n",
            "    >>> pd.to_datetime('13000101', format='%Y%m%d', errors='coerce')\n",
            "    NaT\n",
            "    \n",
            "    Passing infer_datetime_format=True can often-times speedup a parsing\n",
            "    if its not an ISO8601 format exactly, but in a regular format.\n",
            "    \n",
            "    >>> s = pd.Series(['3/11/2000', '3/12/2000', '3/13/2000'] * 1000)\n",
            "    >>> s.head()\n",
            "    0    3/11/2000\n",
            "    1    3/12/2000\n",
            "    2    3/13/2000\n",
            "    3    3/11/2000\n",
            "    4    3/12/2000\n",
            "    dtype: object\n",
            "    \n",
            "    >>> %timeit pd.to_datetime(s, infer_datetime_format=True)  # doctest: +SKIP\n",
            "    100 loops, best of 3: 10.4 ms per loop\n",
            "    \n",
            "    >>> %timeit pd.to_datetime(s, infer_datetime_format=False)  # doctest: +SKIP\n",
            "    1 loop, best of 3: 471 ms per loop\n",
            "    \n",
            "    Using a unix epoch time\n",
            "    \n",
            "    >>> pd.to_datetime(1490195805, unit='s')\n",
            "    Timestamp('2017-03-22 15:16:45')\n",
            "    >>> pd.to_datetime(1490195805433502912, unit='ns')\n",
            "    Timestamp('2017-03-22 15:16:45.433502912')\n",
            "    \n",
            "    .. warning:: For float arg, precision rounding might happen. To prevent\n",
            "        unexpected behavior use a fixed-width exact type.\n",
            "    \n",
            "    Using a non-unix epoch origin\n",
            "    \n",
            "    >>> pd.to_datetime([1, 2, 3], unit='D',\n",
            "    ...                origin=pd.Timestamp('1960-01-01'))\n",
            "    DatetimeIndex(['1960-01-02', '1960-01-03', '1960-01-04'], dtype='datetime64[ns]', freq=None)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSMOAZnomiY9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "23ba7a15-8897-40a8-b4b1-d7b4c06fb60f"
      },
      "source": [
        "help(Week)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class Week in module isoweek:\n",
            "\n",
            "class Week(Week)\n",
            " |  A Week represents a period of 7 days starting with a Monday.\n",
            " |  Weeks are identified by a year and week number within the year.\n",
            " |  This corresponds to the read-only attributes 'year' and 'week'.\n",
            " |  \n",
            " |  Week 1 of a year is defined to be the first week with 4 or more days in\n",
            " |  January.  The preceeding week is either week 52 or 53 of the\n",
            " |  preceeding year.\n",
            " |  \n",
            " |  Week objects are tuples, and thus immutable, with an interface\n",
            " |  similar to the standard datetime.date class.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Week\n",
            " |      Week\n",
            " |      builtins.tuple\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __add__(self, other)\n",
            " |      Adding integers to a Week gives the week that many number of weeks into the future.\n",
            " |      Adding with datetime.timedelta is also supported.\n",
            " |  \n",
            " |  __repr__(self)\n",
            " |      Return a string like \"isoweek.Week(2011, 35)\".\n",
            " |  \n",
            " |  __str__(self)\n",
            " |      Return a ISO formatted week string like \"2011W08\".\n",
            " |  \n",
            " |  __sub__(self, other)\n",
            " |      Subtracting two weeks give the number of weeks between them as an integer.\n",
            " |      Subtracting an integer gives another Week in the past.\n",
            " |  \n",
            " |  contains(self, day)\n",
            " |      Check if the given datetime.date falls within the week\n",
            " |  \n",
            " |  day(self, num)\n",
            " |      Return the given day of week as a date object.  Day 0 is the Monday.\n",
            " |  \n",
            " |  days(self)\n",
            " |      Return the 7 days of the week as a list (of datetime.date objects)\n",
            " |  \n",
            " |  friday(self)\n",
            " |      Return the fifth day the week as a date object\n",
            " |  \n",
            " |  isoformat = __str__(self)\n",
            " |  \n",
            " |  monday(self)\n",
            " |      Return the first day of the week as a date object\n",
            " |  \n",
            " |  replace(self, year=None, week=None)\n",
            " |      Return a Week with either the year or week attribute value replaced\n",
            " |  \n",
            " |  saturday(self)\n",
            " |      Return the sixth day the week as a date object\n",
            " |  \n",
            " |  sunday(self)\n",
            " |      Return the last day the week as a date object\n",
            " |  \n",
            " |  thursday(self)\n",
            " |      Return the fourth day the week as a date object\n",
            " |  \n",
            " |  toordinal(self)\n",
            " |      Return the proleptic Gregorian ordinal the week, where January 1 of year 1 starts the first week.\n",
            " |  \n",
            " |  tuesday(self)\n",
            " |      Return the second day the week as a date object\n",
            " |  \n",
            " |  wednesday(self)\n",
            " |      Return the third day the week as a date object\n",
            " |  \n",
            " |  year_week(self)\n",
            " |      Return a regular tuple containing the (year, week)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  fromordinal(ordinal) from builtins.type\n",
            " |      Return the week corresponding to the proleptic Gregorian ordinal,\n",
            " |      where January 1 of year 1 starts the week with ordinal 1.\n",
            " |  \n",
            " |  fromstring(isostring) from builtins.type\n",
            " |      Return a week initialized from an ISO formatted string like \"2011W08\" or \"2011-W08\".\n",
            " |  \n",
            " |  last_week_of_year(year) from builtins.type\n",
            " |      Return the last week of the given year.\n",
            " |      This week with either have week-number 52 or 53.\n",
            " |      \n",
            " |      This will be the same as Week(year+1, 0), but will even work for\n",
            " |      year 9999 where this expression would overflow.\n",
            " |      \n",
            " |      The first week of a given year is simply Week(year, 1), so there\n",
            " |      is no dedicated classmethod for that.\n",
            " |  \n",
            " |  thisweek() from builtins.type\n",
            " |      Return the current week (local time).\n",
            " |  \n",
            " |  weeks_of_year(year) from builtins.type\n",
            " |      Return an iterator over the weeks of the given year.\n",
            " |      Years have either 52 or 53 weeks.\n",
            " |  \n",
            " |  withdate(date) from builtins.type\n",
            " |      Return the week that contains the given datetime.date\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods defined here:\n",
            " |  \n",
            " |  __new__(cls, year, week)\n",
            " |      Initialize a Week tuple with the given year and week number.\n",
            " |      \n",
            " |      The week number does not have to be within range.  The numbers\n",
            " |      will be normalized if not.  The year must be within the range\n",
            " |      1 to 9999.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  max = isoweek.Week(9999, 52)\n",
            " |  \n",
            " |  min = isoweek.Week(1, 1)\n",
            " |  \n",
            " |  resolution = datetime.timedelta(7)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from Week:\n",
            " |  \n",
            " |  __getnewargs__(self)\n",
            " |      Return self as a plain tuple.  Used by copy and pickle.\n",
            " |  \n",
            " |  _asdict(self)\n",
            " |      Return a new OrderedDict which maps field names to their values.\n",
            " |  \n",
            " |  _replace(_self, **kwds)\n",
            " |      Return a new Week object replacing specified fields with new values\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from Week:\n",
            " |  \n",
            " |  _make(iterable, new=<built-in method __new__ of type object at 0x9d43a0>, len=<built-in function len>) from builtins.type\n",
            " |      Make a new Week object from a sequence or iterable\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from Week:\n",
            " |  \n",
            " |  year\n",
            " |      Alias for field number 0\n",
            " |  \n",
            " |  week\n",
            " |      Alias for field number 1\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from Week:\n",
            " |  \n",
            " |  _fields = ('year', 'week')\n",
            " |  \n",
            " |  _source = \"from builtins import property as _property, tupl..._itemget...\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from builtins.tuple:\n",
            " |  \n",
            " |  __contains__(self, key, /)\n",
            " |      Return key in self.\n",
            " |  \n",
            " |  __eq__(self, value, /)\n",
            " |      Return self==value.\n",
            " |  \n",
            " |  __ge__(self, value, /)\n",
            " |      Return self>=value.\n",
            " |  \n",
            " |  __getattribute__(self, name, /)\n",
            " |      Return getattr(self, name).\n",
            " |  \n",
            " |  __getitem__(self, key, /)\n",
            " |      Return self[key].\n",
            " |  \n",
            " |  __gt__(self, value, /)\n",
            " |      Return self>value.\n",
            " |  \n",
            " |  __hash__(self, /)\n",
            " |      Return hash(self).\n",
            " |  \n",
            " |  __iter__(self, /)\n",
            " |      Implement iter(self).\n",
            " |  \n",
            " |  __le__(self, value, /)\n",
            " |      Return self<=value.\n",
            " |  \n",
            " |  __len__(self, /)\n",
            " |      Return len(self).\n",
            " |  \n",
            " |  __lt__(self, value, /)\n",
            " |      Return self<value.\n",
            " |  \n",
            " |  __mul__(self, value, /)\n",
            " |      Return self*value.\n",
            " |  \n",
            " |  __ne__(self, value, /)\n",
            " |      Return self!=value.\n",
            " |  \n",
            " |  __rmul__(self, value, /)\n",
            " |      Return value*self.\n",
            " |  \n",
            " |  count(...)\n",
            " |      T.count(value) -> integer -- return number of occurrences of value\n",
            " |  \n",
            " |  index(...)\n",
            " |      T.index(value, [start, [stop]]) -> integer -- return first index of value.\n",
            " |      Raises ValueError if the value is not present.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3k5l7XCTuDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from isoweek import Week\n",
        "for df in (joined,joined_test):\n",
        "    df[\"Promo2Since\"] = pd.to_datetime(df.apply(lambda x: Week(\n",
        "        x.Promo2SinceYear, x.Promo2SinceWeek).monday(), axis=1))\n",
        "    df[\"Promo2Days\"] = df.Date.subtract(df[\"Promo2Since\"]).dt.days"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMxS7licmDfx",
        "colab_type": "text"
      },
      "source": [
        "\\# 테이블에서 월요일을 뽑아내 Since에, 날짜들을 뽑아내 days에 저장한다\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyXs1EqfTuDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for df in (joined,joined_test):\n",
        "    df.loc[df.Promo2Days<0, \"Promo2Days\"] = 0\n",
        "    df.loc[df.Promo2SinceYear<1990, \"Promo2Days\"] = 0\n",
        "    df[\"Promo2Weeks\"] = df[\"Promo2Days\"]//7\n",
        "    df.loc[df.Promo2Weeks<0, \"Promo2Weeks\"] = 0\n",
        "    df.loc[df.Promo2Weeks>25, \"Promo2Weeks\"] = 25\n",
        "    df.Promo2Weeks.unique()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXiGh64Ym8TZ",
        "colab_type": "text"
      },
      "source": [
        "\\# 년, 일, 월에 대한 설정을 마찬가지로 수행\n",
        "\\# 주일은 총 날짜/7의 값으로"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MQHLKJRTuDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "joined.to_pickle(PATH/'joined')\n",
        "joined_test.to_pickle(PATH/'joined_test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjU84OZdnIxa",
        "colab_type": "text"
      },
      "source": [
        "\\# pickle로 바꿔 저장"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLUtmSwOTuDw",
        "colab_type": "text"
      },
      "source": [
        "## Durations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhPS2Oe8TuDw",
        "colab_type": "text"
      },
      "source": [
        "It is common when working with time series data to extract data that explains relationships across rows as opposed to columns, e.g.:\n",
        "* Running averages\n",
        "* Time until next event\n",
        "* Time since last event\n",
        "\n",
        "This is often difficult to do with most table manipulation frameworks, since they are designed to work with relationships across columns. As such, we've created a class to handle this type of data.\n",
        "\n",
        "We'll define a function `get_elapsed` for cumulative counting across a sorted dataframe. Given a particular field `fld` to monitor, this function will start tracking time since the last occurrence of that field. When the field is seen again, the counter is set to zero.\n",
        "\n",
        "Upon initialization, this will result in datetime na's until the field is encountered. This is reset every time a new store is seen. We'll see how to use this shortly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjM8YjUNTuDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_elapsed(fld, pre):\n",
        "    day1 = np.timedelta64(1, 'D')\n",
        "    last_date = np.datetime64()\n",
        "    last_store = 0\n",
        "    res = []\n",
        "\n",
        "    for s,v,d in zip(df.Store.values,df[fld].values, df.Date.values):\n",
        "        if s != last_store:\n",
        "            last_date = np.datetime64()\n",
        "            last_store = s\n",
        "        if v: last_date = d\n",
        "        res.append(((d-last_date).astype('timedelta64[D]') / day1))\n",
        "    df[pre+fld] = res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WyjcjXNnVmQ",
        "colab_type": "text"
      },
      "source": [
        "\\# 다음까지 걸리는 시간을 측정할 함수 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8G2qsA6TuDy",
        "colab_type": "text"
      },
      "source": [
        "We'll be applying this to a subset of columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVjFzd7uTuDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = [\"Date\", \"Store\", \"Promo\", \"StateHoliday\", \"SchoolHoliday\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEjh4ZRPTuDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df = train[columns]\n",
        "df = train[columns].append(test[columns])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DbHNG4mTuD1",
        "colab_type": "text"
      },
      "source": [
        "Let's walk through an example.\n",
        "\n",
        "Say we're looking at School Holiday. We'll first sort by Store, then Date, and then call `add_elapsed('SchoolHoliday', 'After')`:\n",
        "This will apply to each row with School Holiday:\n",
        "* A applied to every row of the dataframe in order of store and date\n",
        "* Will add to the dataframe the days since seeing a School Holiday\n",
        "* If we sort in the other direction, this will count the days until another holiday."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKEzNaZFn7iE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "797432a2-86d4-4052-d0a1-3c2c05f786b3"
      },
      "source": [
        "help(df.sort_values)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on method sort_values in module pandas.core.frame:\n",
            "\n",
            "sort_values(by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last', ignore_index=False) method of pandas.core.frame.DataFrame instance\n",
            "    Sort by the values along either axis.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "            by : str or list of str\n",
            "                Name or list of names to sort by.\n",
            "    \n",
            "                - if `axis` is 0 or `'index'` then `by` may contain index\n",
            "                  levels and/or column labels.\n",
            "                - if `axis` is 1 or `'columns'` then `by` may contain column\n",
            "                  levels and/or index labels.\n",
            "    \n",
            "                .. versionchanged:: 0.23.0\n",
            "    \n",
            "                   Allow specifying index or column level names.\n",
            "    axis : {0 or 'index', 1 or 'columns'}, default 0\n",
            "         Axis to be sorted.\n",
            "    ascending : bool or list of bool, default True\n",
            "         Sort ascending vs. descending. Specify list for multiple sort\n",
            "         orders.  If this is a list of bools, must match the length of\n",
            "         the by.\n",
            "    inplace : bool, default False\n",
            "         If True, perform operation in-place.\n",
            "    kind : {'quicksort', 'mergesort', 'heapsort'}, default 'quicksort'\n",
            "         Choice of sorting algorithm. See also ndarray.np.sort for more\n",
            "         information.  `mergesort` is the only stable algorithm. For\n",
            "         DataFrames, this option is only applied when sorting on a single\n",
            "         column or label.\n",
            "    na_position : {'first', 'last'}, default 'last'\n",
            "         Puts NaNs at the beginning if `first`; `last` puts NaNs at the\n",
            "         end.\n",
            "    ignore_index : bool, default False\n",
            "         If True, the resulting axis will be labeled 0, 1, …, n - 1.\n",
            "    \n",
            "         .. versionadded:: 1.0.0\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    sorted_obj : DataFrame or None\n",
            "        DataFrame with sorted values if inplace=False, None otherwise.\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> df = pd.DataFrame({\n",
            "    ...     'col1': ['A', 'A', 'B', np.nan, 'D', 'C'],\n",
            "    ...     'col2': [2, 1, 9, 8, 7, 4],\n",
            "    ...     'col3': [0, 1, 9, 4, 2, 3],\n",
            "    ... })\n",
            "    >>> df\n",
            "        col1 col2 col3\n",
            "    0   A    2    0\n",
            "    1   A    1    1\n",
            "    2   B    9    9\n",
            "    3   NaN  8    4\n",
            "    4   D    7    2\n",
            "    5   C    4    3\n",
            "    \n",
            "    Sort by col1\n",
            "    \n",
            "    >>> df.sort_values(by=['col1'])\n",
            "        col1 col2 col3\n",
            "    0   A    2    0\n",
            "    1   A    1    1\n",
            "    2   B    9    9\n",
            "    5   C    4    3\n",
            "    4   D    7    2\n",
            "    3   NaN  8    4\n",
            "    \n",
            "    Sort by multiple columns\n",
            "    \n",
            "    >>> df.sort_values(by=['col1', 'col2'])\n",
            "        col1 col2 col3\n",
            "    1   A    1    1\n",
            "    0   A    2    0\n",
            "    2   B    9    9\n",
            "    5   C    4    3\n",
            "    4   D    7    2\n",
            "    3   NaN  8    4\n",
            "    \n",
            "    Sort Descending\n",
            "    \n",
            "    >>> df.sort_values(by='col1', ascending=False)\n",
            "        col1 col2 col3\n",
            "    4   D    7    2\n",
            "    5   C    4    3\n",
            "    2   B    9    9\n",
            "    0   A    2    0\n",
            "    1   A    1    1\n",
            "    3   NaN  8    4\n",
            "    \n",
            "    Putting NAs first\n",
            "    \n",
            "    >>> df.sort_values(by='col1', ascending=False, na_position='first')\n",
            "        col1 col2 col3\n",
            "    3   NaN  8    4\n",
            "    4   D    7    2\n",
            "    5   C    4    3\n",
            "    2   B    9    9\n",
            "    0   A    2    0\n",
            "    1   A    1    1\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmFdvJldTuD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fld = 'SchoolHoliday'\n",
        "df = df.sort_values(['Store', 'Date'])\n",
        "get_elapsed(fld, 'After')\n",
        "df = df.sort_values(['Store', 'Date'], ascending=[True, False])\n",
        "get_elapsed(fld, 'Before')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZ1nKoWMnj9-",
        "colab_type": "text"
      },
      "source": [
        "\\# 가계, 날짜별로 정렬 후 함수를 적용\n",
        "\n",
        "Q : ascending = [True, False]면 True로 적용된것인지 False로 적용된것인지 모르겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGRSazH5TuD2",
        "colab_type": "text"
      },
      "source": [
        "We'll do this for two more fields."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li6cdYFSTuD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fld = 'StateHoliday'\n",
        "df = df.sort_values(['Store', 'Date'])\n",
        "get_elapsed(fld, 'After')\n",
        "df = df.sort_values(['Store', 'Date'], ascending=[True, False])\n",
        "get_elapsed(fld, 'Before')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq3qecXmTuD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fld = 'Promo'\n",
        "df = df.sort_values(['Store', 'Date'])\n",
        "get_elapsed(fld, 'After')\n",
        "df = df.sort_values(['Store', 'Date'], ascending=[True, False])\n",
        "get_elapsed(fld, 'Before')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuDTrCleodDt",
        "colab_type": "text"
      },
      "source": [
        "\\# 각각의 필드에 대해서 같은 적용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jniJf9VPTuD5",
        "colab_type": "text"
      },
      "source": [
        "We're going to set the active index to Date."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgAp5z94ojt-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "adba6e8e-d338-45fd-cd94-ed44dc186d64"
      },
      "source": [
        "help(df.set_index)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on method set_index in module pandas.core.frame:\n",
            "\n",
            "set_index(keys, drop=True, append=False, inplace=False, verify_integrity=False) method of pandas.core.frame.DataFrame instance\n",
            "    Set the DataFrame index using existing columns.\n",
            "    \n",
            "    Set the DataFrame index (row labels) using one or more existing\n",
            "    columns or arrays (of the correct length). The index can replace the\n",
            "    existing index or expand on it.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    keys : label or array-like or list of labels/arrays\n",
            "        This parameter can be either a single column key, a single array of\n",
            "        the same length as the calling DataFrame, or a list containing an\n",
            "        arbitrary combination of column keys and arrays. Here, \"array\"\n",
            "        encompasses :class:`Series`, :class:`Index`, ``np.ndarray``, and\n",
            "        instances of :class:`~collections.abc.Iterator`.\n",
            "    drop : bool, default True\n",
            "        Delete columns to be used as the new index.\n",
            "    append : bool, default False\n",
            "        Whether to append columns to existing index.\n",
            "    inplace : bool, default False\n",
            "        Modify the DataFrame in place (do not create a new object).\n",
            "    verify_integrity : bool, default False\n",
            "        Check the new index for duplicates. Otherwise defer the check until\n",
            "        necessary. Setting to False will improve the performance of this\n",
            "        method.\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    DataFrame\n",
            "        Changed row labels.\n",
            "    \n",
            "    See Also\n",
            "    --------\n",
            "    DataFrame.reset_index : Opposite of set_index.\n",
            "    DataFrame.reindex : Change to new indices or expand indices.\n",
            "    DataFrame.reindex_like : Change to same indices as other DataFrame.\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> df = pd.DataFrame({'month': [1, 4, 7, 10],\n",
            "    ...                    'year': [2012, 2014, 2013, 2014],\n",
            "    ...                    'sale': [55, 40, 84, 31]})\n",
            "    >>> df\n",
            "       month  year  sale\n",
            "    0      1  2012    55\n",
            "    1      4  2014    40\n",
            "    2      7  2013    84\n",
            "    3     10  2014    31\n",
            "    \n",
            "    Set the index to become the 'month' column:\n",
            "    \n",
            "    >>> df.set_index('month')\n",
            "           year  sale\n",
            "    month\n",
            "    1      2012    55\n",
            "    4      2014    40\n",
            "    7      2013    84\n",
            "    10     2014    31\n",
            "    \n",
            "    Create a MultiIndex using columns 'year' and 'month':\n",
            "    \n",
            "    >>> df.set_index(['year', 'month'])\n",
            "                sale\n",
            "    year  month\n",
            "    2012  1     55\n",
            "    2014  4     40\n",
            "    2013  7     84\n",
            "    2014  10    31\n",
            "    \n",
            "    Create a MultiIndex using an Index and a column:\n",
            "    \n",
            "    >>> df.set_index([pd.Index([1, 2, 3, 4]), 'year'])\n",
            "             month  sale\n",
            "       year\n",
            "    1  2012  1      55\n",
            "    2  2014  4      40\n",
            "    3  2013  7      84\n",
            "    4  2014  10     31\n",
            "    \n",
            "    Create a MultiIndex using two Series:\n",
            "    \n",
            "    >>> s = pd.Series([1, 2, 3, 4])\n",
            "    >>> df.set_index([s, s**2])\n",
            "          month  year  sale\n",
            "    1 1       1  2012    55\n",
            "    2 4       4  2014    40\n",
            "    3 9       7  2013    84\n",
            "    4 16     10  2014    31\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-pgbpTNTuD6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.set_index(\"Date\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1RTfWbeohAr",
        "colab_type": "text"
      },
      "source": [
        "\\# 인덱스 설정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9DiJnw5TuD8",
        "colab_type": "text"
      },
      "source": [
        "Then set null values from elapsed field calculations to 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HgFZaKqTuD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "columns = ['SchoolHoliday', 'StateHoliday', 'Promo']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7GNKEdsTuD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for o in ['Before', 'After']:\n",
        "    for p in columns:\n",
        "        a = o+p\n",
        "        df[a] = df[a].fillna(0).astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE38keXepA6p",
        "colab_type": "text"
      },
      "source": [
        "\\# 날짜 인덱스로 접근, Na값을 0으로 채움"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsL8Dx_WTuD_",
        "colab_type": "text"
      },
      "source": [
        "Next we'll demonstrate window functions in pandas to calculate rolling quantities.\n",
        "\n",
        "Here we're sorting by date (`sort_index()`) and counting the number of events of interest (`sum()`) defined in `columns` in the following week (`rolling()`), grouped by Store (`groupby()`). We do the same in the opposite direction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKidZV_DpZI4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7ea1349e-2eee-40fa-cce5-eb4df19fe26f"
      },
      "source": [
        "help(df.groupby)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on method groupby in module pandas.core.frame:\n",
            "\n",
            "groupby(by=None, axis=0, level=None, as_index:bool=True, sort:bool=True, group_keys:bool=True, squeeze:bool=False, observed:bool=False) -> 'groupby_generic.DataFrameGroupBy' method of pandas.core.frame.DataFrame instance\n",
            "    Group DataFrame using a mapper or by a Series of columns.\n",
            "    \n",
            "    A groupby operation involves some combination of splitting the\n",
            "    object, applying a function, and combining the results. This can be\n",
            "    used to group large amounts of data and compute operations on these\n",
            "    groups.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    by : mapping, function, label, or list of labels\n",
            "        Used to determine the groups for the groupby.\n",
            "        If ``by`` is a function, it's called on each value of the object's\n",
            "        index. If a dict or Series is passed, the Series or dict VALUES\n",
            "        will be used to determine the groups (the Series' values are first\n",
            "        aligned; see ``.align()`` method). If an ndarray is passed, the\n",
            "        values are used as-is determine the groups. A label or list of\n",
            "        labels may be passed to group by the columns in ``self``. Notice\n",
            "        that a tuple is interpreted as a (single) key.\n",
            "    axis : {0 or 'index', 1 or 'columns'}, default 0\n",
            "        Split along rows (0) or columns (1).\n",
            "    level : int, level name, or sequence of such, default None\n",
            "        If the axis is a MultiIndex (hierarchical), group by a particular\n",
            "        level or levels.\n",
            "    as_index : bool, default True\n",
            "        For aggregated output, return object with group labels as the\n",
            "        index. Only relevant for DataFrame input. as_index=False is\n",
            "        effectively \"SQL-style\" grouped output.\n",
            "    sort : bool, default True\n",
            "        Sort group keys. Get better performance by turning this off.\n",
            "        Note this does not influence the order of observations within each\n",
            "        group. Groupby preserves the order of rows within each group.\n",
            "    group_keys : bool, default True\n",
            "        When calling apply, add group keys to index to identify pieces.\n",
            "    squeeze : bool, default False\n",
            "        Reduce the dimensionality of the return type if possible,\n",
            "        otherwise return a consistent type.\n",
            "    observed : bool, default False\n",
            "        This only applies if any of the groupers are Categoricals.\n",
            "        If True: only show observed values for categorical groupers.\n",
            "        If False: show all values for categorical groupers.\n",
            "    \n",
            "        .. versionadded:: 0.23.0\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    DataFrameGroupBy\n",
            "        Returns a groupby object that contains information about the groups.\n",
            "    \n",
            "    See Also\n",
            "    --------\n",
            "    resample : Convenience method for frequency conversion and resampling\n",
            "        of time series.\n",
            "    \n",
            "    Notes\n",
            "    -----\n",
            "    See the `user guide\n",
            "    <https://pandas.pydata.org/pandas-docs/stable/groupby.html>`_ for more.\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> df = pd.DataFrame({'Animal': ['Falcon', 'Falcon',\n",
            "    ...                               'Parrot', 'Parrot'],\n",
            "    ...                    'Max Speed': [380., 370., 24., 26.]})\n",
            "    >>> df\n",
            "       Animal  Max Speed\n",
            "    0  Falcon      380.0\n",
            "    1  Falcon      370.0\n",
            "    2  Parrot       24.0\n",
            "    3  Parrot       26.0\n",
            "    >>> df.groupby(['Animal']).mean()\n",
            "            Max Speed\n",
            "    Animal\n",
            "    Falcon      375.0\n",
            "    Parrot       25.0\n",
            "    \n",
            "    **Hierarchical Indexes**\n",
            "    \n",
            "    We can groupby different levels of a hierarchical index\n",
            "    using the `level` parameter:\n",
            "    \n",
            "    >>> arrays = [['Falcon', 'Falcon', 'Parrot', 'Parrot'],\n",
            "    ...           ['Captive', 'Wild', 'Captive', 'Wild']]\n",
            "    >>> index = pd.MultiIndex.from_arrays(arrays, names=('Animal', 'Type'))\n",
            "    >>> df = pd.DataFrame({'Max Speed': [390., 350., 30., 20.]},\n",
            "    ...                   index=index)\n",
            "    >>> df\n",
            "                    Max Speed\n",
            "    Animal Type\n",
            "    Falcon Captive      390.0\n",
            "           Wild         350.0\n",
            "    Parrot Captive       30.0\n",
            "           Wild          20.0\n",
            "    >>> df.groupby(level=0).mean()\n",
            "            Max Speed\n",
            "    Animal\n",
            "    Falcon      370.0\n",
            "    Parrot       25.0\n",
            "    >>> df.groupby(level=\"Type\").mean()\n",
            "             Max Speed\n",
            "    Type\n",
            "    Captive      210.0\n",
            "    Wild         185.0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6Xja-HQpuX2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "616f43f6-31a1-4b55-cf7e-c22accd6f493"
      },
      "source": [
        "help(df.rolling)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on method rolling in module pandas.core.generic:\n",
            "\n",
            "rolling(window, min_periods=None, center=False, win_type=None, on=None, axis=0, closed=None) method of pandas.core.frame.DataFrame instance\n",
            "    Provide rolling window calculations.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    window : int, offset, or BaseIndexer subclass\n",
            "        Size of the moving window. This is the number of observations used for\n",
            "        calculating the statistic. Each window will be a fixed size.\n",
            "    \n",
            "        If its an offset then this will be the time period of each window. Each\n",
            "        window will be a variable sized based on the observations included in\n",
            "        the time-period. This is only valid for datetimelike indexes.\n",
            "    \n",
            "        If a BaseIndexer subclass is passed, calculates the window boundaries\n",
            "        based on the defined ``get_window_bounds`` method. Additional rolling\n",
            "        keyword arguments, namely `min_periods`, `center`, and\n",
            "        `closed` will be passed to `get_window_bounds`.\n",
            "    min_periods : int, default None\n",
            "        Minimum number of observations in window required to have a value\n",
            "        (otherwise result is NA). For a window that is specified by an offset,\n",
            "        `min_periods` will default to 1. Otherwise, `min_periods` will default\n",
            "        to the size of the window.\n",
            "    center : bool, default False\n",
            "        Set the labels at the center of the window.\n",
            "    win_type : str, default None\n",
            "        Provide a window type. If ``None``, all points are evenly weighted.\n",
            "        See the notes below for further information.\n",
            "    on : str, optional\n",
            "        For a DataFrame, a datetime-like column or MultiIndex level on which\n",
            "        to calculate the rolling window, rather than the DataFrame's index.\n",
            "        Provided integer column is ignored and excluded from result since\n",
            "        an integer index is not used to calculate the rolling window.\n",
            "    axis : int or str, default 0\n",
            "    closed : str, default None\n",
            "        Make the interval closed on the 'right', 'left', 'both' or\n",
            "        'neither' endpoints.\n",
            "        For offset-based windows, it defaults to 'right'.\n",
            "        For fixed windows, defaults to 'both'. Remaining cases not implemented\n",
            "        for fixed windows.\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    a Window or Rolling sub-classed for the particular operation\n",
            "    \n",
            "    See Also\n",
            "    --------\n",
            "    expanding : Provides expanding transformations.\n",
            "    ewm : Provides exponential weighted functions.\n",
            "    \n",
            "    Notes\n",
            "    -----\n",
            "    By default, the result is set to the right edge of the window. This can be\n",
            "    changed to the center of the window by setting ``center=True``.\n",
            "    \n",
            "    To learn more about the offsets & frequency strings, please see `this link\n",
            "    <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__.\n",
            "    \n",
            "    The recognized win_types are:\n",
            "    \n",
            "    * ``boxcar``\n",
            "    * ``triang``\n",
            "    * ``blackman``\n",
            "    * ``hamming``\n",
            "    * ``bartlett``\n",
            "    * ``parzen``\n",
            "    * ``bohman``\n",
            "    * ``blackmanharris``\n",
            "    * ``nuttall``\n",
            "    * ``barthann``\n",
            "    * ``kaiser`` (needs beta)\n",
            "    * ``gaussian`` (needs std)\n",
            "    * ``general_gaussian`` (needs power, width)\n",
            "    * ``slepian`` (needs width)\n",
            "    * ``exponential`` (needs tau), center is set to None.\n",
            "    \n",
            "    If ``win_type=None`` all points are evenly weighted. To learn more about\n",
            "    different window types see `scipy.signal window functions\n",
            "    <https://docs.scipy.org/doc/scipy/reference/signal.html#window-functions>`__.\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    \n",
            "    >>> df = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]})\n",
            "    >>> df\n",
            "         B\n",
            "    0  0.0\n",
            "    1  1.0\n",
            "    2  2.0\n",
            "    3  NaN\n",
            "    4  4.0\n",
            "    \n",
            "    Rolling sum with a window length of 2, using the 'triang'\n",
            "    window type.\n",
            "    \n",
            "    >>> df.rolling(2, win_type='triang').sum()\n",
            "         B\n",
            "    0  NaN\n",
            "    1  0.5\n",
            "    2  1.5\n",
            "    3  NaN\n",
            "    4  NaN\n",
            "    \n",
            "    Rolling sum with a window length of 2, using the 'gaussian'\n",
            "    window type (note how we need to specify std).\n",
            "    \n",
            "    >>> df.rolling(2, win_type='gaussian').sum(std=3)\n",
            "              B\n",
            "    0       NaN\n",
            "    1  0.986207\n",
            "    2  2.958621\n",
            "    3       NaN\n",
            "    4       NaN\n",
            "    \n",
            "    Rolling sum with a window length of 2, min_periods defaults\n",
            "    to the window length.\n",
            "    \n",
            "    >>> df.rolling(2).sum()\n",
            "         B\n",
            "    0  NaN\n",
            "    1  1.0\n",
            "    2  3.0\n",
            "    3  NaN\n",
            "    4  NaN\n",
            "    \n",
            "    Same as above, but explicitly set the min_periods\n",
            "    \n",
            "    >>> df.rolling(2, min_periods=1).sum()\n",
            "         B\n",
            "    0  0.0\n",
            "    1  1.0\n",
            "    2  3.0\n",
            "    3  2.0\n",
            "    4  4.0\n",
            "    \n",
            "    A ragged (meaning not-a-regular frequency), time-indexed DataFrame\n",
            "    \n",
            "    >>> df = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]},\n",
            "    ...                   index = [pd.Timestamp('20130101 09:00:00'),\n",
            "    ...                            pd.Timestamp('20130101 09:00:02'),\n",
            "    ...                            pd.Timestamp('20130101 09:00:03'),\n",
            "    ...                            pd.Timestamp('20130101 09:00:05'),\n",
            "    ...                            pd.Timestamp('20130101 09:00:06')])\n",
            "    \n",
            "    >>> df\n",
            "                           B\n",
            "    2013-01-01 09:00:00  0.0\n",
            "    2013-01-01 09:00:02  1.0\n",
            "    2013-01-01 09:00:03  2.0\n",
            "    2013-01-01 09:00:05  NaN\n",
            "    2013-01-01 09:00:06  4.0\n",
            "    \n",
            "    Contrasting to an integer rolling window, this will roll a variable\n",
            "    length window corresponding to the time period.\n",
            "    The default for min_periods is 1.\n",
            "    \n",
            "    >>> df.rolling('2s').sum()\n",
            "                           B\n",
            "    2013-01-01 09:00:00  0.0\n",
            "    2013-01-01 09:00:02  1.0\n",
            "    2013-01-01 09:00:03  3.0\n",
            "    2013-01-01 09:00:05  NaN\n",
            "    2013-01-01 09:00:06  4.0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYo7XPyaTuD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bwd = df[['Store']+columns].sort_index().groupby(\"Store\").rolling(7, min_periods=1).sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGJyd__PTuEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fwd = df[['Store']+columns].sort_index(ascending=False\n",
        "                                      ).groupby(\"Store\").rolling(7, min_periods=1).sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sXXs1cNpNZc",
        "colab_type": "text"
      },
      "source": [
        "\\# before / after에 대한 데이터프레임을 정렬하고 같은 가계의 값을 합쳐서 각각 저장\n",
        "\n",
        "Q : rolling window calculation이 무엇을 의미하는지 모르겠습니다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLhc2AdjTuEE",
        "colab_type": "text"
      },
      "source": [
        "Next we want to drop the Store indices grouped together in the window function.\n",
        "\n",
        "Often in pandas, there is an option to do this in place. This is time and memory efficient when working with large datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4V1f99UTuEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bwd.drop('Store',1,inplace=True)\n",
        "bwd.reset_index(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxYcb1MSTuEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fwd.drop('Store',1,inplace=True)\n",
        "fwd.reset_index(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_q3tROHTuEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.reset_index(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKTI4osAqPVH",
        "colab_type": "text"
      },
      "source": [
        "\\# store제거 후 인덱스 리셋"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpYW9vJITuEJ",
        "colab_type": "text"
      },
      "source": [
        "Now we'll merge these values onto the df."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frOYhjyuTuEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.merge(bwd, 'left', ['Date', 'Store'], suffixes=['', '_bw'])\n",
        "df = df.merge(fwd, 'left', ['Date', 'Store'], suffixes=['', '_fw'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvDmOk6VTuEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.drop(columns,1,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gST0CTBnqfw8",
        "colab_type": "text"
      },
      "source": [
        "\\# before 와 after의 값을 데이터 프레임에 합침"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prQ5qQMuTuEM",
        "colab_type": "code",
        "outputId": "437bfb8d-05d3-47c9-e7f7-1b2f3dfa218f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Store</th>\n",
              "      <th>AfterSchoolHoliday</th>\n",
              "      <th>BeforeSchoolHoliday</th>\n",
              "      <th>AfterStateHoliday</th>\n",
              "      <th>BeforeStateHoliday</th>\n",
              "      <th>AfterPromo</th>\n",
              "      <th>BeforePromo</th>\n",
              "      <th>SchoolHoliday_bw</th>\n",
              "      <th>StateHoliday_bw</th>\n",
              "      <th>Promo_bw</th>\n",
              "      <th>SchoolHoliday_fw</th>\n",
              "      <th>StateHoliday_fw</th>\n",
              "      <th>Promo_fw</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2015-09-17</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>105</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2015-09-16</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>104</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2015-09-15</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>103</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2015-09-14</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>102</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2015-09-13</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>101</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Date  Store  ...  StateHoliday_fw  Promo_fw\n",
              "0 2015-09-17      1  ...              0.0       1.0\n",
              "1 2015-09-16      1  ...              0.0       2.0\n",
              "2 2015-09-15      1  ...              0.0       3.0\n",
              "3 2015-09-14      1  ...              0.0       4.0\n",
              "4 2015-09-13      1  ...              0.0       4.0\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppsVUqo3qpq5",
        "colab_type": "text"
      },
      "source": [
        "\\# 출력"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOqRyEuFTuEN",
        "colab_type": "text"
      },
      "source": [
        "It's usually a good idea to back up large tables of extracted / wrangled features before you join them onto another one, that way you can go back to it easily if you need to make changes to it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Rj_-L6Cq_Nh",
        "colab_type": "text"
      },
      "source": [
        "\\# 백업"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAIHxU1eTuEO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_pickle(PATH/'df')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe3JKn2WTuEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[\"Date\"] = pd.to_datetime(df.Date)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-ONAVlTTuEQ",
        "colab_type": "code",
        "outputId": "d2d4205d-119b-4992-c4d8-e0f0fe55ff9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Date', 'Store', 'AfterSchoolHoliday', 'BeforeSchoolHoliday',\n",
              "       'AfterStateHoliday', 'BeforeStateHoliday', 'AfterPromo', 'BeforePromo',\n",
              "       'SchoolHoliday_bw', 'StateHoliday_bw', 'Promo_bw', 'SchoolHoliday_fw',\n",
              "       'StateHoliday_fw', 'Promo_fw'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CpbWstfTuER",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "joined = pd.read_pickle(PATH/'joined')\n",
        "joined_test = pd.read_pickle(PATH/f'joined_test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ux1w1uajTuET",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "joined = join_df(joined, df, ['Store', 'Date'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZKa8QrvTuEU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "joined_test = join_df(joined_test, df, ['Store', 'Date'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWc3OjGHTuEX",
        "colab_type": "text"
      },
      "source": [
        "The authors also removed all instances where the store had zero sale / was closed. We speculate that this may have cost them a higher standing in the competition. One reason this may be the case is that a little exploratory data analysis reveals that there are often periods where stores are closed, typically for refurbishment. Before and after these periods, there are naturally spikes in sales that one might expect. By ommitting this data from their training, the authors gave up the ability to leverage information about these periods to predict this otherwise volatile behavior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6jZEgk1TuEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "joined = joined[joined.Sales!=0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yht20sW-rQxl",
        "colab_type": "text"
      },
      "source": [
        "\\# 매출이 0, 폐점인 경우를 제거"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToTgNyxFTuEZ",
        "colab_type": "text"
      },
      "source": [
        "We'll back this up as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJB-5AfkTuEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "joined.reset_index(inplace=True)\n",
        "joined_test.reset_index(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHkfj2x1TuEc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "joined.to_pickle(PATH/'train_clean')\n",
        "joined_test.to_pickle(PATH/'test_clean')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz-s8u5WTuEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\\# 전체적으로 rossman_data_clean의 내용은 csv 여러개에 나누어져 있는 내용을 하나의 데이터 프레임 테이블로 변환하고 다듬는 과정이다."
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}